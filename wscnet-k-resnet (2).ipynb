{"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["0V4Pp2DrB_nX","ZbwC74P9VsGS","FHU2GKXzjivC"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"372fd7dc785e484e809775192f98dc68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6c309b800064c71850790dcf40b9026","IPY_MODEL_a25df6989c294a5bbd1c347dde1e5b1a","IPY_MODEL_9e0e096005274f7ba9052f9d102b69b2"],"layout":"IPY_MODEL_c25aaf1662864905989926199903b4e5"}},"e6c309b800064c71850790dcf40b9026":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9838d48b263f4848a8b7b8e301f0b639","placeholder":"​","style":"IPY_MODEL_b1d01b3493d84c00b2c9ad43753a1725","value":"  0%"}},"a25df6989c294a5bbd1c347dde1e5b1a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbdf50e56c5f4d299864f907423ba9c2","max":22,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60ac861eaa414b01b226aba5561683d6","value":0}},"9e0e096005274f7ba9052f9d102b69b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bce3ee283065434c9b1b3603a84c96c3","placeholder":"​","style":"IPY_MODEL_4ed985cfa57a481abf38c7af3bac107b","value":" 0/22 [00:00&lt;?, ?it/s]"}},"c25aaf1662864905989926199903b4e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9838d48b263f4848a8b7b8e301f0b639":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1d01b3493d84c00b2c9ad43753a1725":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbdf50e56c5f4d299864f907423ba9c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60ac861eaa414b01b226aba5561683d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bce3ee283065434c9b1b3603a84c96c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ed985cfa57a481abf38c7af3bac107b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8191887,"sourceType":"datasetVersion","datasetId":4851416},{"sourceId":8268209,"sourceType":"datasetVersion","datasetId":4908547}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Packages","metadata":{"id":"miDiIE9oQ3-C"}},{"cell_type":"code","source":"import os\n!nvidia-smi\n%config Completer.use_jedi = False # 运行后在敲语句时按tab即可查看补全\n#pip install --upgrade d2l   在 Console 中运行导入 李沐老师开发的 d2l 库\nfrom d2l import torch as d2l\n#pip install einops  导入能优雅处理张量的 einops 库\n#from einops import rearrange, repeat, reduce","metadata":{"id":"qlvTExNXvwWt","outputId":"096c1f71-df43-4238-8e34-7842b1c58d73","execution":{"iopub.status.busy":"2024-06-06T08:51:05.515300Z","iopub.execute_input":"2024-06-06T08:51:05.516163Z","iopub.status.idle":"2024-06-06T08:51:09.804609Z","shell.execute_reply.started":"2024-06-06T08:51:05.516124Z","shell.execute_reply":"2024-06-06T08:51:09.803774Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Thu Jun  6 08:51:06 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   72C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import math\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torch.nn.init as init # 高斯分布初始化\n# import keras.backend as K\n\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR # 时间衰减学习率\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\n# 导入tqdm库的auto模块中的tqdm函数，它是一个快速、可扩展的Python进度条库，可以在长循环中添加一个进度提示信息，用户可以实时了解程序的运行进度。\nimport random\n\n_exp_name = \"WSCNet_sample\"\n\nmyseed = 114514  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed) #设置随机种子打乱\nrandom.seed(myseed) #设置随机种子打乱\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"id":"cqHnhKjRQ3gQ","execution":{"iopub.status.busy":"2024-06-06T08:51:09.806342Z","iopub.execute_input":"2024-06-06T08:51:09.807353Z","iopub.status.idle":"2024-06-06T08:51:09.864876Z","shell.execute_reply.started":"2024-06-06T08:51:09.807324Z","shell.execute_reply":"2024-06-06T08:51:09.864148Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Transforms\n\n这些增强操作可以帮助你的模型在训练时看到更多样化的数据，从而提高其泛化能力。","metadata":{"id":"0V4Pp2DrB_nX"}},{"cell_type":"code","source":"# 将输入图像预处理加工，防止训练过拟合\n# 在测试时通常不需要这些增强操作，因为你想评估模型在未经修改的数据上的性能。\ntest_tfm = transforms.Compose([\n    transforms.Resize((448, 448)),\n    # 图像太大也会产生内存超限报错\n    transforms.ToTensor(),\n])\ntrain_tfm = transforms.Compose([\n    #transforms.RandomVerticalFlip(p=0.2), # 随机垂直翻转图像，概率为0.4。\n    transforms.RandomResizedCrop(size=(448, 448), scale=(0.08, 1.0), ratio=(3./4., 4./3.)),\n    transforms.RandomHorizontalFlip(p=0.5), # 随机水平翻转图像，概率为0.5。\n    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # 随机调整图像的亮度、对比度、饱和度和色相。\n    # ToTensor() should be the last one of the transforms. 最后转为张量\n    transforms.ToTensor(),\n])","metadata":{"id":"t5W5WD5XB--J","execution":{"iopub.status.busy":"2024-06-06T08:51:09.865966Z","iopub.execute_input":"2024-06-06T08:51:09.866285Z","iopub.status.idle":"2024-06-06T08:51:09.872352Z","shell.execute_reply.started":"2024-06-06T08:51:09.866253Z","shell.execute_reply":"2024-06-06T08:51:09.871515Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# MyDataset","metadata":{"id":"QalrVXh9HGVv"}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, path, tfm = test_tfm, files = None):\n        super(MyDataset).__init__()\n        \n        self.my_dict = {'anger': '0', 'disgust': '1', 'fear': '2', 'joy': '3', \n                        'sadness': '4', 'surprise': '5'}\n        self.files = []\n        self.transform = tfm\n\n        path = os.path.join('/kaggle/input/emotionroi/training_testing_split', path) \n        \n        # 检查文件是否存在\n        if os.path.exists(path):\n            # 打开文件\n            with open(path, 'r') as file:\n                # 读取文件的每一行\n                for line in file:\n                    line = line.strip().replace('_', '/') \n                    # strip() 方法用于移除字符串头尾的空白字符，包括空格、制表符、换行符等\n                    # 将下划线 \"_\" 替换为斜杠 \"/\"\n                    self.files.append(line)\n        else:\n            print(f\"文件不存在：{path}\")\n        \n        if files != None:\n            self.files = files\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self,idx): # 通过下标找文件\n                \n        label = int(self.my_dict[self.files[idx].split(\"/\")[0]]) # label 就是 int 类型！！！\n\n        fname = os.path.join('/kaggle/input/emotionroi/images', self.files[idx])\n\n        im = Image.open(fname) # 开启图像文件\n        im = self.transform(im) # transform转化文件\n\n        return im,label # 返回图像和标签","metadata":{"id":"8ZWGvjlmHDMB","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-06-06T08:51:09.874459Z","iopub.execute_input":"2024-06-06T08:51:09.874809Z","iopub.status.idle":"2024-06-06T08:51:09.885681Z","shell.execute_reply.started":"2024-06-06T08:51:09.874762Z","shell.execute_reply":"2024-06-06T08:51:09.884795Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"# Construct train and valid datasets.\n# The argument \"loader\" tells how torchvision reads the data.\npath = 'training.txt'\ntrain_set = MyDataset(path, tfm=train_tfm) # 训练集的图像变换\ntrain_loader = DataLoader(train_set, batch_size=train_set.__len__(), shuffle=True, num_workers=0, pin_memory=True)    \n# torch.Size([1386, 3, 256, 256])\n# torch.Size([1386])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:51:09.886863Z","iopub.execute_input":"2024-06-06T08:51:09.887428Z","iopub.status.idle":"2024-06-06T08:51:09.902687Z","shell.execute_reply.started":"2024-06-06T08:51:09.887383Z","shell.execute_reply":"2024-06-06T08:51:09.901975Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# WSCNet Model","metadata":{"id":"TNKJIh9XVyfs"}},{"cell_type":"code","source":"class Self_Attention(nn.Module):\n    def __init__(self, dim, dk, dv, dropout):\n        super(Self_Attention, self).__init__()\n        self.scale = dk ** -0.5\n        self.q = nn.Linear(dim, dk)\n        self.k = nn.Linear(dim, dk)\n        self.v = nn.Linear(dim, dv)\n        self.dropout = nn.Dropout(dropout)\n        init.normal_(self.q.weight, mean=0, std=math.sqrt(2.0/dim))\n        init.normal_(self.k.weight, mean=0, std=math.sqrt(2.0/dim))\n        init.normal_(self.v.weight, mean=0, std=math.sqrt(2.0/dim))\n    \n    def forward(self, x):\n        # x = [Batch, x[1], dim] = [Batch, token数量, token内部向量长度]\n        q = self.q(x)\n        k = self.k(x)\n        v = self.v(x)\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        x = self.dropout(attn) @ v # @是矩阵乘法，*是逐元素相乘\n        return x # [Batch, x[1], dv] \n\nclass Cross_spatial_pooling(nn.Module):\n    def __init__(self):\n        super(Cross_spatial_pooling, self).__init__()\n\n    def forward(self, out, Batch, C, K):\n        out1 = nn.AdaptiveMaxPool2d(output_size=(1, 1))(out) # 全局最大池化得到 [B, kC, 1, 1]\n        out1 = out1.view(Batch, -1) # 转化向量, -1表示自动计算长度，[B, kC, 1, 1] -> [B, kC*1*1]\n        \n        out1 = out1.view(Batch, C, K)\n        # 首先，我们将张量重新形状为 [B, C, K]，这样我们可以沿着最后一个维度取平均值\n        \n        #out1 = out1.mean(dim=2)\n        #然后，我们沿着最后一个维度取平均值，得到形状为 [B, C] 的张量\n        return out1\n\nclass Sentiment_map_Coupling(nn.Module):\n    def __init__(self):\n        super(Sentiment_map_Coupling, self).__init__()\n\n    def forward(self, Res_out, out, out1, Batch, C, K, N, H, W):\n        Vi = out1.unsqueeze(-1).unsqueeze(-1) # 情感权重 Vi = [B, C, 1, 1]\n        # 然后，我们将 out 分割为 C 个部分，每个部分对应一个类别\n        # 这可以通过重塑 out 为 [B, C, K, H, W] 来实现，重塑yyds！\n        out_reshaped = out.view(Batch, C, K, H, W)  # [B, C, K, H, W]\n        # 接下来，我们对每个类别的 K 个通道取平均值，这可以通过在 K 维度上应用 mean 函数来实现\n        St_map_subgraph = out_reshaped.mean(dim=2)  # 情感子图 = [B, C, H, W]\n        Vi = Vi.repeat(1, 1, St_map_subgraph.size(2), St_map_subgraph.size(3))\n        \n        # 现在，将 out1 拓展后的 Vi 作为权值乘上 St_map_subgraph\n        St_map = St_map_subgraph * Vi  # [B, C, H, W]\n        \n        St_map = St_map.sum(dim=1)  # [B, H, W]\n        # 最后，我们可以通过 sum 在 C 维度上合并所有的类别贡献\n        St_map = St_map.unsqueeze(1).repeat(1, N, 1, 1)\n        # 层数不足则要用 unsqueeze + repeat，每个参数对应每个维度的倍数，会复制数据\n\n        U_out = Res_out * St_map  # [B, N, H, W]\n        # 耦合，两个通道数拼接起来 2N [B, 2N, H, W] \n        out2 = torch.cat((U_out, Res_out), dim = 1)\n        out2 = nn.AdaptiveAvgPool2d(output_size=(1, 1))(out2) # [B, 2N, 1, 1]\n        out2 = out2.view((Batch, -1)) # [B, 2N]\n        # 报错显示Linear未放入device，要手动放入\n        return out2\n    \nclass WSCNet(nn.Module):\n    def __init__(self, K = 4, C = 6):\n        super().__init__()\n\n        self.K = K\n        self.C = C\n        self.Conv2d = nn.Conv2d(in_channels=2048, out_channels=self.K * self.C, kernel_size=1)\n        self.linear = nn.Linear(in_features=4096, out_features=self.C)\n        init.normal_(self.Conv2d.weight, mean=0, std=math.sqrt(1.0/2048))\n        init.normal_(self.linear.weight, mean=0, std=math.sqrt(2.0/4096))\n        self.Attention = Self_Attention(dim=4, dk=2, dv=4, dropout=0.5)\n        \n        self.Conv1x1 = nn.Sequential(\n            self.Conv2d,\n            nn.BatchNorm2d(self.K * self.C),\n            nn.ReLU(inplace=True),\n        )\n        self.Cross_spatial_pooling = Cross_spatial_pooling()\n        self.Sentiment_map_Coupling = Sentiment_map_Coupling()\n\n    def forward(self, Res_out):\n\n        Batch = Res_out.shape[0]\n        N = Res_out.shape[1]\n        H = Res_out.shape[2]\n        W = Res_out.shape[3]\n        \n        # 检测分支————————————————————————————————————————————————————————\n        out = self.Conv1x1(Res_out) # [B, kC, h, w]   F'矩阵\n        out1 = self.Cross_spatial_pooling(out, Batch, self.C, self.K) # [B, C, K]\n        \n        out1 = self.Attention(out1) # [B, C, K]\n        out1 = out1.mean(dim=-1) # [B, C]\n        \n        D_branch = out1\n        \n        # 分类分支————————————————————————————————————————————————————————\n        # 使用矩阵运算来代替循环，这样才可以在GPU中加速！！！从每epoch 12分钟到 30 秒......\n        # out 是 shape [B, K*C, H, W] 的张量, out1 是 shape [B, C] 的张量\n        out2 = self.Sentiment_map_Coupling(Res_out, out, out1, Batch, self.C, self.K, N, H, W)\n        C_branch = self.linear(out2) #[B, C]\n\n        return D_branch, C_branch # 返回两个 [B, C] 张量","metadata":{"id":"S1xeKwD1QD_E","execution":{"iopub.status.busy":"2024-06-06T08:51:09.903728Z","iopub.execute_input":"2024-06-06T08:51:09.903984Z","iopub.status.idle":"2024-06-06T08:51:09.927835Z","shell.execute_reply.started":"2024-06-06T08:51:09.903962Z","shell.execute_reply":"2024-06-06T08:51:09.926912Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Net and Loss and Accuracy","metadata":{}},{"cell_type":"code","source":"get_loss = nn.CrossEntropyLoss() # 交叉熵损失函数\n\ndef get_net():\n    pretrained_weights = torch.load('/kaggle/input/resnet101-tranlearn/resnet101-5d3b4d8f.pth')\n    resnet101 = torchvision.models.resnet101(weights = pretrained_weights)# 预训练\n    resnet101 = nn.Sequential(*list(resnet101.children())[:-2])\n    net = nn.Sequential(resnet101, WSCNet(K=4, C=6))\n    return net\n#get_net()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:51:09.929028Z","iopub.execute_input":"2024-06-06T08:51:09.929433Z","iopub.status.idle":"2024-06-06T08:51:09.938192Z","shell.execute_reply.started":"2024-06-06T08:51:09.929402Z","shell.execute_reply":"2024-06-06T08:51:09.937278Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_acc(features, labels):\n    # 为了在取对数时进一步稳定该值，将小于1的值设置为1\n    _, predicted = torch.max(features, 1)\n    correct = predicted.eq(labels).cpu().sum().numpy()\n    return 100.0 * correct / features.size(0)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:51:09.939427Z","iopub.execute_input":"2024-06-06T08:51:09.940128Z","iopub.status.idle":"2024-06-06T08:51:09.945474Z","shell.execute_reply.started":"2024-06-06T08:51:09.940097Z","shell.execute_reply":"2024-06-06T08:51:09.944674Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Train function","metadata":{}},{"cell_type":"code","source":"def train(net, train_features, train_labels, valid_features, valid_labels,\n          num_epochs, batch_size):\n    \n    train_ls, train_acc, valid_ls, valid_acc = [], [], [], [] # 训练和验证\n    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n    if valid_features is not None:\n        valid_iter = d2l.load_array((valid_features, valid_labels), batch_size)\n    \n    # 这里使用的是Adam优化算法\n    optimizer = torch.optim.SGD(\n        [\n            \n            {\"params\": net[0].parameters(), \"lr\": 0.00005},\n            {\"params\": net[1].Attention.parameters(), \"lr\": 0.001},\n            {\"params\": net[1].Conv1x1.parameters(), \"lr\": 0.001},\n            {\"params\": net[1].linear.parameters(), \"lr\": 0.01},\n        ],\n        weight_decay=5e-3, momentum=0.9)#momentum=0.9 Adam_lr = 1e-5\n    # 先用 Adam 后用 SGD 可能有更好效果 lr = 0.0003，Adam的效果比SGD好多了，默认 lr = 0.0003 最佳？\n    \n    scheduler = StepLR(optimizer, step_size=6, gamma=0.1) # step_size 适当缩小\n    # 创建一个 StepLR scheduler，每10个epoch将学习率乘以0.1\n    \n    for epoch in range(num_epochs):\n          \n        avg_loss = 0\n        avg_acc = 0\n        step = 0\n        \n        net.train()\n        for imgs, labels in tqdm(train_iter):\n            \n            imgs = imgs.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            \n            dy, cy = net(imgs)\n            \n            loss1 = get_loss(dy, labels) # label 就是要 int 型！\n            loss2 = get_loss(cy, labels)\n            Loss = loss1 + loss2\n            \n            avg_loss += Loss.item() # 数值都转到cpu，不然会存GPU值，有些运算报错\n            avg_acc += get_acc(cy, labels)\n            step += 1\n            \n            Loss.backward()\n            optimizer.step()\n            \n        train_ls.append(avg_loss/step)\n        train_acc.append(avg_acc/step)      \n            \n        if valid_features is not None:\n            avg_loss = 0\n            avg_acc = 0\n            step = 0\n            \n            net.eval()\n            for imgs, labels in tqdm(valid_iter):\n\n                imgs = imgs.to(device)\n                labels = labels.to(device)\n                \n                with torch.no_grad():\n                    dy, cy = net(imgs)\n\n                loss1 = get_loss(dy, labels)\n                loss2 = get_loss(cy, labels)\n                Loss = loss1 + loss2\n                \n                avg_loss += Loss.item()\n                avg_acc += get_acc(cy, labels)\n                step += 1\n                \n            valid_ls.append(avg_loss/step)\n            valid_acc.append(avg_acc/step)\n            \n        scheduler.step() # 每8个epoch结束后调整学习率, Adam也可以调整\n        current_lr = scheduler.get_last_lr()\n        print(f\"Epoch {epoch + 1}, Current Learning Rate: \\n{current_lr}\")\n        print(f\"train_ls: {train_ls[-1]}\", end = \" \")\n        print(f\"train_acc: {train_acc[-1]}\")\n        \n        if valid_features is not None:\n            print(f\"valid_ls: {valid_ls[-1]}\", end = \" \")\n            print(f\"valid_acc: {valid_acc[-1]}\")\n        \n    return train_ls, valid_ls, train_acc, valid_acc","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-06-06T08:51:09.946621Z","iopub.execute_input":"2024-06-06T08:51:09.946925Z","iopub.status.idle":"2024-06-06T08:51:09.963872Z","shell.execute_reply.started":"2024-06-06T08:51:09.946902Z","shell.execute_reply":"2024-06-06T08:51:09.963126Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# K折交叉验证","metadata":{}},{"cell_type":"code","source":"def get_k_fold_data(k, i, X, y):\n    assert k > 1\n    fold_size = X.shape[0] // k\n    X_train, y_train = None, None\n    \n    for j in range(k):\n        idx = slice(j * fold_size, (j + 1) * fold_size) # 创造切片对象\n        X_part, y_part = X[idx, :, :, :], y[idx]\n        \n        if j == i:\n            X_valid, y_valid = X_part, y_part\n        elif X_train is None:\n            X_train, y_train = X_part, y_part\n        else:\n            X_train = torch.cat([X_train, X_part], 0)\n            y_train = torch.cat([y_train, y_part], 0)\n            \n    return X_train, y_train, X_valid, y_valid","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-06-06T08:51:09.966526Z","iopub.execute_input":"2024-06-06T08:51:09.966811Z","iopub.status.idle":"2024-06-06T08:51:09.975171Z","shell.execute_reply.started":"2024-06-06T08:51:09.966788Z","shell.execute_reply":"2024-06-06T08:51:09.974264Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def k_fold(net, k, X_train, y_train, num_epochs, batch_size):\n    \n    train_l_sum, valid_l_sum, train_acc_sum, valid_acc_sum = 0, 0, 0, 0\n    \n    for i in range(k):\n        data = get_k_fold_data(k, i, X_train, y_train) # K折处理数据\n        \n        train_ls, valid_ls, train_acc, valid_acc = train(net, *data, num_epochs, batch_size) # 训练函数\n        \n        train_l_sum += train_ls[-1]\n        train_acc_sum += train_acc[-1]\n        valid_l_sum += valid_ls[-1]\n        valid_acc_sum += valid_acc[-1]\n        \n        if i == 0:\n            print(\"大折\")\n            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, train_acc, valid_ls, valid_acc], # 合并两组数据到一个列表\n                     xlabel='epoch', ylabel='num', xlim=[1, num_epochs],\n                     legend=['train_ls', 'train_acc', 'valid_ls', 'valid_acc'], yscale='log')\n            \n        print(f'折{i + 1}，训练log CEloss: {float(train_ls[-1])}')\n        print(f'验证log CEloss: {float(valid_ls[-1])}')\n        \n        print(f'折{i + 1}，训练log accuracy: {float(train_acc[-1])}')\n        print(f'验证log accuracy: {float(valid_acc[-1])}')\n        \n    return train_l_sum / k, valid_l_sum / k, train_acc_sum / k, valid_acc_sum / k","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-06-06T08:51:09.976554Z","iopub.execute_input":"2024-06-06T08:51:09.976883Z","iopub.status.idle":"2024-06-06T08:51:09.986445Z","shell.execute_reply.started":"2024-06-06T08:51:09.976855Z","shell.execute_reply":"2024-06-06T08:51:09.985487Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Start Training","metadata":{}},{"cell_type":"code","source":"train_imgs, train_labels = None, None\nfor batch in tqdm(train_loader):\n    train_imgs, train_labels = batch","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:51:09.987668Z","iopub.execute_input":"2024-06-06T08:51:09.988357Z","iopub.status.idle":"2024-06-06T08:51:27.054322Z","shell.execute_reply.started":"2024-06-06T08:51:09.988326Z","shell.execute_reply":"2024-06-06T08:51:27.053288Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b728f1bbbb464c8053824f122f89bb"}},"metadata":{}}]},{"cell_type":"code","source":"# The number of batch size. 第一维\nbatch_size = 22\n\n# The number of training epochs.\nnum_epochs = 16\n\nk = 1 # K折\n\nnet = get_net().to(device)# 得到训练网络！\n\ndata = [train_imgs, train_labels, None, None]\ntrain_ls, valid_ls, train_acc, valid_acc = train(net, *data, num_epochs, batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:51:27.055521Z","iopub.execute_input":"2024-06-06T08:51:27.055811Z","iopub.status.idle":"2024-06-06T09:15:34.713194Z","shell.execute_reply.started":"2024-06-06T08:51:27.055786Z","shell.execute_reply":"2024-06-06T09:15:34.712179Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19af98d98570422b831f0a97bbe734bf"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Current Learning Rate: \n[5e-05, 0.001, 0.001, 0.01]\ntrain_ls: 4.232204520513141 train_acc: 34.34343434343434\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a9ecaa6f4604eb996031d3bd79bb7fd"}},"metadata":{}},{"name":"stdout","text":"Epoch 2, Current Learning Rate: \n[5e-05, 0.001, 0.001, 0.01]\ntrain_ls: 3.546270586195446 train_acc: 48.05194805194807\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d1beca278db4f2399efee62b83177ad"}},"metadata":{}},{"name":"stdout","text":"Epoch 3, Current Learning Rate: \n[5e-05, 0.001, 0.001, 0.01]\ntrain_ls: 3.441809616391621 train_acc: 50.793650793650784\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a2238bda580400691a79cab04101003"}},"metadata":{}},{"name":"stdout","text":"Epoch 4, Current Learning Rate: \n[5e-05, 0.001, 0.001, 0.01]\ntrain_ls: 3.3826951904902383 train_acc: 52.38095238095236\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0df976fedabc4fb8b73cb94fbdc2ad82"}},"metadata":{}},{"name":"stdout","text":"Epoch 5, Current Learning Rate: \n[5e-05, 0.001, 0.001, 0.01]\ntrain_ls: 3.1073183559236073 train_acc: 59.595959595959556\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"988de2f991e34ff0aed54bb977aead70"}},"metadata":{}},{"name":"stdout","text":"Epoch 6, Current Learning Rate: \n[5e-06, 0.0001, 0.0001, 0.001]\ntrain_ls: 2.7984546441880482 train_acc: 65.007215007215\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"555a916e318b4adc9d834d537ea43c09"}},"metadata":{}},{"name":"stdout","text":"Epoch 7, Current Learning Rate: \n[5e-06, 0.0001, 0.0001, 0.001]\ntrain_ls: 2.4316055623311845 train_acc: 76.11832611832614\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"409f527e621e4532a35b54acf135856f"}},"metadata":{}},{"name":"stdout","text":"Epoch 8, Current Learning Rate: \n[5e-06, 0.0001, 0.0001, 0.001]\ntrain_ls: 2.3277310076214017 train_acc: 79.43722943722948\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52d76b0328d466b937d4813516faf74"}},"metadata":{}},{"name":"stdout","text":"Epoch 9, Current Learning Rate: \n[5e-06, 0.0001, 0.0001, 0.001]\ntrain_ls: 2.328959480164543 train_acc: 80.37518037518045\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c28f5fd088a44a0a4cbf9859bb0d502"}},"metadata":{}},{"name":"stdout","text":"Epoch 10, Current Learning Rate: \n[5e-06, 0.0001, 0.0001, 0.001]\ntrain_ls: 2.316531593837435 train_acc: 81.16883116883116\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e80e58590545959c6ea35af684d61e"}},"metadata":{}},{"name":"stdout","text":"Epoch 11, Current Learning Rate: \n[5e-06, 0.0001, 0.0001, 0.001]\ntrain_ls: 2.260740030379522 train_acc: 83.98268398268398\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e18ca8024b44a15833c8cb1b47e8847"}},"metadata":{}},{"name":"stdout","text":"Epoch 12, Current Learning Rate: \n[5.000000000000001e-07, 1e-05, 1e-05, 0.0001]\ntrain_ls: 2.25217238305107 train_acc: 82.97258297258298\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d60530ac5c114ea1b06525cb2485243a"}},"metadata":{}},{"name":"stdout","text":"Epoch 13, Current Learning Rate: \n[5.000000000000001e-07, 1e-05, 1e-05, 0.0001]\ntrain_ls: 2.2819685406155057 train_acc: 83.62193362193364\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4ebaae4a12446c7836f055da4280f28"}},"metadata":{}},{"name":"stdout","text":"Epoch 14, Current Learning Rate: \n[5.000000000000001e-07, 1e-05, 1e-05, 0.0001]\ntrain_ls: 2.240186479356554 train_acc: 84.63203463203462\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"530ca1406e2a400a980e6d2eb9068329"}},"metadata":{}},{"name":"stdout","text":"Epoch 15, Current Learning Rate: \n[5.000000000000001e-07, 1e-05, 1e-05, 0.0001]\ntrain_ls: 2.2459323973882768 train_acc: 84.48773448773451\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ee4041ac8514cf1b11cc40d9fc01269"}},"metadata":{}},{"name":"stdout","text":"Epoch 16, Current Learning Rate: \n[5.000000000000001e-07, 1e-05, 1e-05, 0.0001]\ntrain_ls: 2.2470588305639843 train_acc: 84.12698412698411\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataloader for Test","metadata":{"id":"rw2y9Uhw7wr2"}},{"cell_type":"code","source":"path = 'testing.txt'\n#path = 'training.txt'\ntest_set = MyDataset(path, tfm=test_tfm) # 测试\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, \n                         num_workers=0, pin_memory=True)","metadata":{"id":"zceT1uQyqJfO","execution":{"iopub.status.busy":"2024-06-06T09:15:34.714596Z","iopub.execute_input":"2024-06-06T09:15:34.714954Z","iopub.status.idle":"2024-06-06T09:15:34.726747Z","shell.execute_reply.started":"2024-06-06T09:15:34.714920Z","shell.execute_reply":"2024-06-06T09:15:34.726058Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{"id":"G31uyjpvVaVP"}},{"cell_type":"code","source":"avg_loss = 0\navg_acc = 0\nstep = 0\n\ntest_ls, test_acc = [], []\n\nnet.eval()\nfor batch in tqdm(test_loader):\n\n    imgs, labels = batch\n    imgs = imgs.to(device)\n    labels = labels.to(device)\n\n    with torch.no_grad():\n        dy, cy = net(imgs)\n\n    loss1 = get_loss(dy, labels)\n    loss2 = get_loss(cy, labels)\n    Loss = loss1 + loss2\n\n    avg_loss += Loss.item()\n    avg_acc += get_acc(cy, labels)\n    step += 1\n\ntest_ls.append(avg_loss/step)\ntest_acc.append(avg_acc/step)\nprint(f\"test_ls: {test_ls[-1]}\", end = \" \")\nprint(f\"test_acc: {test_acc[-1]}\")","metadata":{"id":"bpLtxx5FVaVP","execution":{"iopub.status.busy":"2024-06-06T09:15:34.727849Z","iopub.execute_input":"2024-06-06T09:15:34.728758Z","iopub.status.idle":"2024-06-06T09:15:51.783769Z","shell.execute_reply.started":"2024-06-06T09:15:34.728726Z","shell.execute_reply":"2024-06-06T09:15:51.782855Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/27 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75c305b53d3341f28b44e5c339e94ee7"}},"metadata":{}},{"name":"stdout","text":"test_ls: 3.3000311763198287 test_acc: 52.86195286195287\n","output_type":"stream"}]}]}